"use strict";(self["webpackChunkDANER"]=self["webpackChunkDANER"]||[]).push([[707],{4707:(e,a,t)=>{t.r(a),t.d(a,{default:()=>D});var l=t(3673),n=t(2323);const s=(0,l.HX)("data-v-5404722e");(0,l.dD)("data-v-5404722e");const r={class:"lm"},o={class:"row items-center no-wrap"},d=(0,l.Wm)("div",{class:"col"},[(0,l.Wm)("div",{class:"text-h6"},"BERT"),(0,l.Wm)("div",{class:"text-subtitle2"},"Dec 12, 2019")],-1),i={class:"col-auto"},c=(0,l.Uk)("Remove Card"),u=(0,l.Uk)("Send Feedback"),m=(0,l.Uk)("Share"),f=(0,l.Uk)("Finetune"),p=(0,l.Uk)("Deploy"),g={class:"row items-center no-wrap"},W=(0,l.Wm)("div",{class:"col"},[(0,l.Wm)("div",{class:"text-h6"},"GPT3"),(0,l.Wm)("div",{class:"text-subtitle2"},"May 28, 2020")],-1),h={class:"col-auto"},b=(0,l.Uk)("Remove Card"),_=(0,l.Uk)("Send Feedback"),v=(0,l.Uk)("Share"),k=(0,l.Uk)("Finetune"),w=(0,l.Uk)("Deploy");(0,l.Cn)();const y=s(((e,a,t,y,T,E)=>{const G=(0,l.up)("q-item-section"),B=(0,l.up)("q-item"),R=(0,l.up)("q-list"),P=(0,l.up)("q-menu"),U=(0,l.up)("q-btn"),q=(0,l.up)("q-card-section"),Q=(0,l.up)("q-separator"),Z=(0,l.up)("q-card-actions"),C=(0,l.up)("q-card"),S=(0,l.up)("q-page");return(0,l.wg)(),(0,l.j4)(S,{padding:""},{default:s((()=>[(0,l.Wm)("div",r,[(0,l.Wm)(C,{bordered:"",class:"my-card"},{default:s((()=>[(0,l.Wm)(q,{class:"bg-primary"},{default:s((()=>[(0,l.Wm)("div",o,[d,(0,l.Wm)("div",i,[(0,l.Wm)(U,{color:"grey-7",round:"",flat:"",icon:"more_vert"},{default:s((()=>[(0,l.Wm)(P,{cover:"","auto-close":""},{default:s((()=>[(0,l.Wm)(R,null,{default:s((()=>[(0,l.Wm)(B,{clickable:""},{default:s((()=>[(0,l.Wm)(G,null,{default:s((()=>[c])),_:1})])),_:1}),(0,l.Wm)(B,{clickable:""},{default:s((()=>[(0,l.Wm)(G,null,{default:s((()=>[u])),_:1})])),_:1}),(0,l.Wm)(B,{clickable:""},{default:s((()=>[(0,l.Wm)(G,null,{default:s((()=>[m])),_:1})])),_:1})])),_:1})])),_:1})])),_:1})])])])),_:1}),(0,l.Wm)(q,{class:"bg-grey-1"},{default:s((()=>[(0,l.Uk)((0,n.zw)(y.bert),1)])),_:1}),(0,l.Wm)(Q),(0,l.Wm)(Z,null,{default:s((()=>[(0,l.Wm)(U,{flat:"","no-caps":""},{default:s((()=>[f])),_:1}),(0,l.Wm)(U,{flat:"","no-caps":""},{default:s((()=>[p])),_:1})])),_:1})])),_:1})]),(0,l.Wm)("div",null,[(0,l.Wm)(C,{bordered:"",class:"my-card"},{default:s((()=>[(0,l.Wm)(q,{class:"bg-secondary"},{default:s((()=>[(0,l.Wm)("div",g,[W,(0,l.Wm)("div",h,[(0,l.Wm)(U,{color:"grey-7",round:"",flat:"",icon:"more_vert"},{default:s((()=>[(0,l.Wm)(P,{cover:"","auto-close":""},{default:s((()=>[(0,l.Wm)(R,null,{default:s((()=>[(0,l.Wm)(B,{clickable:""},{default:s((()=>[(0,l.Wm)(G,null,{default:s((()=>[b])),_:1})])),_:1}),(0,l.Wm)(B,{clickable:""},{default:s((()=>[(0,l.Wm)(G,null,{default:s((()=>[_])),_:1})])),_:1}),(0,l.Wm)(B,{clickable:""},{default:s((()=>[(0,l.Wm)(G,null,{default:s((()=>[v])),_:1})])),_:1})])),_:1})])),_:1})])),_:1})])])])),_:1}),(0,l.Wm)(q,{class:"bg-grey-1"},{default:s((()=>[(0,l.Uk)((0,n.zw)(y.gpt3),1)])),_:1}),(0,l.Wm)(Q),(0,l.Wm)(Z,null,{default:s((()=>[(0,l.Wm)(U,{flat:"","no-caps":""},{default:s((()=>[k])),_:1}),(0,l.Wm)(U,{flat:"","no-caps":""},{default:s((()=>[w])),_:1})])),_:1})])),_:1})])])),_:1})})),T={setup(){return{bert:"Bidirectional Encoder Representations from Transformers (BERT) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google.[1][2] As of 2019, Google has been leveraging BERT to better understand user searches.[3]\n\nThe original English-language BERT has two models:[1] (1) the BERTBASE: 12 Encoders with 12 bidirectional self-attention heads, and (2) the BERTLARGE: 24 Encoders with 16 bidirectional self-attention heads. Both models are pre-trained from unlabeled data extracted from the BooksCorpus[4] with 800M words and English Wikipedia with 2,500M words",gpt3:"Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. It is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory.[2] GPT-3's full version has a capacity of 175 billion machine learning parameters. GPT-3, which was introduced in May 2020, and was in beta testing as of July 2020,[3] is part of a trend in natural language processing (NLP) systems of pre-trained language representations.[1] Before the release of GPT-3, the largest language model was Microsoft's Turing NLG, introduced in February 2020, with a capacity of 17 billion parameters - less than a tenth of GPT-3's.[4]"}}};var E=t(4379),G=t(151),B=t(5589),R=t(8240),P=t(811),U=t(7011),q=t(3414),Q=t(2035),Z=t(5869),C=t(9367),S=t(7518),A=t.n(S);T.render=y,T.__scopeId="data-v-5404722e";const D=T;A()(T,"components",{QPage:E.Z,QCard:G.Z,QCardSection:B.Z,QBtn:R.Z,QMenu:P.Z,QList:U.Z,QItem:q.Z,QItemSection:Q.Z,QSeparator:Z.Z,QCardActions:C.Z})}}]);